{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load data: digits 5 to 9, but still label with 0 to 4, \n",
    "# because TensorFlow expects label's integers from 0 to n_classes-1.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to keep only 100 instances per class in the training set \n",
    "# and let's keep only 30 instances per class in the validation set\n",
    "# tesing set is already loaded above\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train Softmax Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Models/yen/mnist_5.ckpt\n",
      "Fully_Connected_Layer_1/kernel:0 with value [[ 0.01931068 -0.06229468 -0.00941171 ...,  0.03909926  0.04221622\n",
      "  -0.04330433]\n",
      " [-0.00933204  0.03915324  0.00364224 ...,  0.03335876  0.02471407\n",
      "  -0.05709484]\n",
      " [ 0.06294347  0.0362412  -0.02793374 ..., -0.01014248  0.00405748\n",
      "   0.01919695]\n",
      " ..., \n",
      " [-0.00440493 -0.08679988  0.01828381 ..., -0.01226822 -0.0290583\n",
      "   0.01330545]\n",
      " [-0.00126087 -0.01126607  0.05368334 ..., -0.01311072 -0.07754496\n",
      "  -0.05414378]\n",
      " [ 0.02870154  0.00207583 -0.06847856 ..., -0.00602869  0.03187896\n",
      "  -0.10222078]]\n",
      "Fully_Connected_Layer_1/bias:0 with value [-0.00559808 -0.02556255 -0.01321042  0.01134591 -0.00392597 -0.00561612\n",
      " -0.00062723  0.00083992  0.00243474  0.00278281 -0.00177744  0.01883522\n",
      "  0.0004022  -0.04384289 -0.01300703 -0.01684944  0.00790067 -0.00461863\n",
      " -0.00275254 -0.01915802 -0.01435215  0.01625893 -0.01254096 -0.00747392\n",
      "  0.01061013  0.00153161  0.01508079 -0.01723556  0.01301864 -0.01697732\n",
      "  0.00834766  0.00085908 -0.00516296  0.01155112 -0.00338811  0.02201465\n",
      " -0.02451535  0.00388774 -0.00649716  0.00101554  0.00516638  0.00726379\n",
      "  0.01102711  0.00088407  0.00050247 -0.00068893  0.00019806  0.00332646\n",
      "  0.00616781  0.00363068 -0.01162461  0.00484433  0.0038688  -0.01354761\n",
      "  0.01229964  0.01150055  0.00518466  0.00026314  0.01512376  0.00827165\n",
      "  0.00256408 -0.01691082  0.00943645  0.00686093  0.01757414 -0.00235671\n",
      " -0.01316527 -0.01220549 -0.00134256 -0.00863948 -0.0070197  -0.00696412\n",
      " -0.02076729  0.01101905 -0.00100935 -0.00289302  0.01816422  0.00700881\n",
      " -0.03267052  0.00034693 -0.01482005 -0.01113124  0.00207882 -0.01063399\n",
      " -0.00398428  0.00736538 -0.02682658 -0.00228685  0.02244285  0.01969626\n",
      " -0.00280013 -0.00162585  0.01426221 -0.00462466 -0.00393267 -0.005966\n",
      "  0.00564401  0.00172102 -0.00136273 -0.02222487  0.01218007  0.00092723\n",
      "  0.00512591 -0.00328966  0.0033117  -0.00694251 -0.00121041 -0.00142081\n",
      " -0.0020606  -0.00997888 -0.01187951 -0.00135419 -0.01523845 -0.01409398\n",
      " -0.00265537  0.0258415  -0.01041225 -0.00575393 -0.00486209  0.00194359\n",
      "  0.00972406 -0.00601681 -0.02316424 -0.00497786 -0.00340494  0.00509176\n",
      " -0.01267503 -0.0006903 ]\n",
      "Fully_Connected_Layer_2/kernel:0 with value [[-0.05797623  0.00040192 -0.17341904 ...,  0.28373003 -0.17601484\n",
      "  -0.01044692]\n",
      " [ 0.03927175 -0.18802243  0.14627248 ...,  0.13495855  0.19747311\n",
      "  -0.22135258]\n",
      " [ 0.06633323  0.15528885  0.05462473 ...,  0.15844071 -0.02901543\n",
      "   0.07455058]\n",
      " ..., \n",
      " [-0.02926307  0.15835471  0.03665823 ...,  0.05488036  0.07170454\n",
      "   0.14617945]\n",
      " [ 0.17113256 -0.15746073  0.05677371 ...,  0.21514146  0.11594405\n",
      "   0.13088483]\n",
      " [-0.02393571 -0.02090634 -0.18416047 ..., -0.02768771  0.10115861\n",
      "  -0.16018128]]\n",
      "Fully_Connected_Layer_2/bias:0 with value [ 0.00079318  0.00429442 -0.00913326  0.00520406 -0.00695541  0.00218822\n",
      " -0.01792059 -0.0066243   0.00674136  0.00529579  0.00601678 -0.00045875\n",
      "  0.01245353  0.00999524 -0.00582072  0.00511287 -0.01425615  0.00523292\n",
      " -0.00011721 -0.00668571 -0.01026684 -0.00164835 -0.0022618   0.00018419\n",
      "  0.00073254 -0.01898249  0.00134694 -0.01907266 -0.01498716 -0.00974528\n",
      " -0.0041907  -0.02019618  0.00356986 -0.00248226 -0.01311713  0.00058846\n",
      "  0.01064751 -0.00633262 -0.00060087  0.00554217 -0.0141924  -0.01037273\n",
      " -0.01789319 -0.00393416  0.01374456  0.01107392 -0.00105391 -0.02219586\n",
      " -0.00393903 -0.00285416 -0.00703092 -0.01160275 -0.00749323 -0.00407037\n",
      "  0.00616297  0.00435638  0.00230227  0.0036279  -0.03277311  0.00573145\n",
      " -0.00771519 -0.00042251 -0.003223    0.00517603 -0.01124096 -0.01156417\n",
      "  0.01902676  0.00722466  0.00687996  0.00504054  0.00854027 -0.01351469\n",
      " -0.00608516 -0.00451906  0.00792576 -0.01378478  0.00828196 -0.01223427\n",
      "  0.00062844 -0.00777706  0.02429413 -0.00089929  0.01342596  0.00734002\n",
      "  0.0063792  -0.00818272  0.00161659  0.00090759  0.00681484 -0.01796906\n",
      "  0.00417454 -0.00448618 -0.00279396  0.00509857 -0.00467968 -0.00518663\n",
      " -0.01673762 -0.00832325  0.00396956  0.02451726 -0.01084516  0.00582889\n",
      "  0.00691125 -0.00982373 -0.00041678  0.00463306  0.00337637  0.00259121\n",
      " -0.00153347 -0.01151997  0.00670623  0.00198005  0.00460496 -0.01548319\n",
      " -0.01457078 -0.00018034 -0.01739762  0.00671206 -0.01324559 -0.00516204\n",
      " -0.00594796 -0.00015806  0.00667767 -0.00503197 -0.00400687  0.00602506\n",
      "  0.00509614 -0.00507511]\n",
      "Fully_Connected_Layer_3/kernel:0 with value [[-0.21560583 -0.27460024 -0.1050993  ...,  0.16171958 -0.05850848\n",
      "   0.03213331]\n",
      " [-0.05814674  0.13301891  0.1886797  ...,  0.08304916  0.26138839\n",
      "   0.07382226]\n",
      " [-0.09944402  0.05948002  0.26937038 ..., -0.19053479  0.0424549\n",
      "   0.30266568]\n",
      " ..., \n",
      " [-0.16639531 -0.20880306 -0.08944021 ...,  0.16479149  0.168439\n",
      "  -0.10653806]\n",
      " [-0.04608197  0.11692449 -0.20841718 ..., -0.20934416  0.11301336\n",
      "  -0.12332051]\n",
      " [ 0.04169199  0.07600179 -0.09382422 ...,  0.0718731  -0.00575832\n",
      "  -0.04771693]]\n",
      "Fully_Connected_Layer_3/bias:0 with value [  4.16299328e-03  -6.67225046e-04  -2.95385066e-03  -1.46655152e-02\n",
      "   2.36238949e-02  -7.43633183e-03  -1.66092310e-02   1.15032112e-02\n",
      "   3.12718388e-04  -7.60109164e-03  -5.29037975e-03  -5.98734617e-03\n",
      "   2.77155591e-03  -4.67564096e-04  -9.31201503e-03  -1.43428706e-03\n",
      "   6.63797895e-04  -8.29185825e-03  -1.12981896e-03  -1.80764385e-02\n",
      "   3.24403751e-03  -1.01384148e-02  -2.89874081e-03  -2.18811724e-02\n",
      "   4.50918125e-03   3.31743900e-03  -7.26343738e-03  -8.25105144e-06\n",
      "   4.77850484e-03  -6.95293862e-03  -8.88691284e-03  -7.37101259e-03\n",
      "  -2.71447178e-04  -5.41664520e-03   9.39557515e-03  -6.16076170e-03\n",
      "   2.37013516e-03  -1.00715961e-02  -1.76987816e-02   3.13546858e-04\n",
      "  -3.79905035e-03  -1.39897950e-02  -4.60334384e-04  -9.03000124e-03\n",
      "  -1.46625023e-02  -9.28340759e-03  -9.47658811e-03   4.76163533e-03\n",
      "   1.40155042e-02   1.19752912e-02   4.26389324e-03  -7.40427151e-03\n",
      "   1.65882625e-03   1.27775026e-02  -8.43846612e-03   7.18082907e-03\n",
      "   4.08431422e-03   9.31163412e-03  -5.22626797e-03  -2.36508921e-02\n",
      "  -5.35087520e-03   1.59253064e-03  -9.38795134e-03   6.69580558e-03\n",
      "  -1.47004062e-02   4.97833127e-03   7.08226673e-03  -6.47827657e-03\n",
      "  -1.14326654e-02   3.84150795e-03   4.77520237e-03  -3.85215343e-03\n",
      "  -1.04570305e-02  -2.26249476e-03   1.83260906e-03   8.29084311e-03\n",
      "   2.10113823e-03  -1.24394372e-02   4.56457250e-02   2.87450966e-03\n",
      "   3.08410265e-03  -1.25495987e-02  -1.37613500e-02   1.00352624e-02\n",
      "  -1.65087953e-02   1.46290360e-04  -1.21286130e-02  -8.28584377e-03\n",
      "   1.01038292e-02  -1.63590405e-02  -1.96905024e-02  -1.56585798e-02\n",
      "   4.58571967e-03   4.97795700e-04  -7.78018078e-03  -2.84224609e-03\n",
      "  -5.14601264e-03  -6.81481883e-03  -2.11953046e-03  -4.52775368e-03\n",
      "   3.61425360e-03  -8.17520078e-03   1.14003720e-04   5.48631046e-03\n",
      "   1.51171563e-02   2.91631767e-03  -6.73577003e-03  -4.80878353e-03\n",
      "   8.90848227e-03  -2.23827796e-04   1.84649434e-02   4.66539990e-03\n",
      "   1.81786101e-02   7.34258071e-03  -4.22663381e-03  -1.24811763e-02\n",
      "  -1.76786929e-02   6.36725104e-04   9.58328717e-04  -1.65957510e-02\n",
      "   5.66205475e-03  -9.80632380e-04   6.64492894e-04   9.31335986e-03\n",
      "  -2.44916161e-03  -5.11820690e-06   4.06283699e-03  -1.15913851e-02]\n",
      "Fully_Connected_Layer_4/kernel:0 with value [[ 0.1805701   0.1588365   0.02328782 ...,  0.08032331  0.00503147\n",
      "   0.02726465]\n",
      " [-0.06041427 -0.12360978  0.1129872  ..., -0.15458539 -0.02128375\n",
      "   0.00849988]\n",
      " [ 0.11350971 -0.27290902 -0.25787508 ..., -0.0887389  -0.16777043\n",
      "  -0.10226952]\n",
      " ..., \n",
      " [-0.10384491 -0.02598054  0.19292121 ...,  0.12524769  0.07653064\n",
      "  -0.10537972]\n",
      " [-0.16727668 -0.23732735 -0.21565738 ..., -0.26111001 -0.14689529\n",
      "   0.02383154]\n",
      " [-0.08897305 -0.08432435  0.06108504 ..., -0.04959979 -0.27240148\n",
      "   0.22537497]]\n",
      "Fully_Connected_Layer_4/bias:0 with value [-0.01690607 -0.0023804  -0.00730634 -0.0060144   0.01083469 -0.02108995\n",
      "  0.01504746  0.0029649  -0.01270931 -0.0040976  -0.00925635  0.01944634\n",
      " -0.00562889 -0.00535876 -0.013139    0.00074541 -0.01226777  0.00101605\n",
      " -0.00620961 -0.00130763  0.01089116 -0.00223802  0.00050897  0.00915073\n",
      " -0.00907971 -0.00969134 -0.00482691 -0.00254611  0.0087014  -0.01040771\n",
      " -0.01176257  0.00348265  0.00672802 -0.00043153 -0.01540609  0.01108778\n",
      "  0.01774141 -0.0050547   0.01205201 -0.00780628  0.00570435 -0.00906339\n",
      "  0.00976653 -0.00497577 -0.00274356  0.00073234  0.01319414 -0.03190813\n",
      "  0.00680698 -0.00384941 -0.00991989  0.02596592 -0.01483145 -0.0007332\n",
      "  0.0029091   0.01564619 -0.00881673 -0.01537109 -0.01452619 -0.0064243\n",
      " -0.00972138  0.01237876  0.00978313  0.00100075  0.00339493  0.00322698\n",
      " -0.01351858  0.01984278 -0.00881543  0.01666981 -0.00453357  0.00411488\n",
      " -0.00415877  0.01536078  0.01772639  0.02002946  0.00821705 -0.00959779\n",
      "  0.01244683  0.00463134  0.00593262 -0.00692825 -0.00810664  0.01269231\n",
      "  0.00522778 -0.00653053 -0.00610716 -0.0109115  -0.0019112  -0.01459939\n",
      "  0.00325588  0.00225562  0.0071581   0.00205411  0.00899515  0.00379852\n",
      " -0.00534231  0.01053119  0.00646278  0.00833512  0.00266465  0.00706118\n",
      "  0.01778658  0.00690096 -0.00138937 -0.00550666  0.00637384  0.00101791\n",
      " -0.00075989 -0.00975067 -0.00714963  0.00276076 -0.02518856  0.00195806\n",
      " -0.02359554 -0.00792751 -0.01301004 -0.00791683 -0.00137234 -0.0111565\n",
      " -0.00578406 -0.02170929  0.00621242 -0.0070843  -0.00325878 -0.00376733\n",
      " -0.02545558  0.01518974]\n",
      "Fully_Connected_Layer_5/kernel:0 with value [[-0.19574673 -0.00846289  0.0042071  ...,  0.04647329 -0.21905883\n",
      "  -0.06103003]\n",
      " [ 0.02446095 -0.02072074 -0.08459818 ..., -0.10311561 -0.2770921\n",
      "  -0.00433225]\n",
      " [ 0.0793001  -0.15191622 -0.1607928  ..., -0.10823143  0.00528874\n",
      "  -0.06261875]\n",
      " ..., \n",
      " [-0.15510872  0.18579096 -0.03549699 ...,  0.0758739   0.13529408\n",
      "  -0.01611337]\n",
      " [-0.23477215 -0.20030312 -0.28957042 ..., -0.07312843  0.10108902\n",
      "  -0.15577306]\n",
      " [-0.14009856 -0.01607902 -0.02100204 ...,  0.15920772 -0.09603255\n",
      "  -0.04930415]]\n",
      "Fully_Connected_Layer_5/bias:0 with value [ 0.00897952  0.0261288  -0.00361738  0.01606107  0.00485677  0.00853961\n",
      " -0.00516724  0.00247857 -0.00781185 -0.00193122 -0.01081287 -0.01470404\n",
      "  0.00635877  0.00620191 -0.00019924 -0.00582032  0.00457808 -0.01220019\n",
      " -0.00379068 -0.00356707  0.01442168 -0.00051579  0.00118961 -0.00340194\n",
      " -0.02066396 -0.01024429 -0.01124032 -0.00698322  0.0046841   0.00430198\n",
      " -0.00242285  0.02279872  0.00739651  0.01360321 -0.00672665  0.01278118\n",
      "  0.00332623  0.02440013  0.02038112 -0.00561231  0.02079753 -0.00430748\n",
      "  0.01058628 -0.01235935  0.00047095 -0.01303371 -0.01206216  0.01318898\n",
      "  0.00231801  0.01143422 -0.00190834 -0.00268907 -0.01154743  0.00836074\n",
      " -0.01377568  0.00645716 -0.00601165  0.00045065 -0.01425749  0.00216457\n",
      " -0.00382474 -0.00226057 -0.00241048 -0.00370066  0.00658345 -0.00351872\n",
      "  0.00250235  0.01554826  0.00095335 -0.00171464  0.0174965   0.00325838\n",
      "  0.01790087  0.01177693  0.00250717  0.01408617  0.02695621  0.01012467\n",
      "  0.01063603 -0.00493528  0.00263604  0.00572919  0.01318403  0.00108651\n",
      " -0.01448138  0.00511423  0.01604828 -0.01461454  0.00405     0.00175665\n",
      "  0.00696565  0.00120135  0.00182556 -0.00077399  0.00791773  0.00908652\n",
      " -0.00408584 -0.00462775  0.0180005   0.00974268  0.01431121  0.00747134\n",
      "  0.00607272  0.0028706  -0.00070032  0.00423119 -0.02181246  0.0046674\n",
      " -0.00887611 -0.01043141 -0.01644635  0.00564378 -0.00183984 -0.00539399\n",
      "  0.00369177 -0.0050337  -0.00444508  0.01595022  0.00658723 -0.00311444\n",
      "  0.01863591  0.00665445 -0.00521275 -0.00766745  0.0031718  -0.00358891\n",
      " -0.0025436  -0.00669243]\n",
      "Softmax_Layer/kernel:0 with value [[  1.76165596e-01   7.26997927e-02   2.48131573e-01  -2.48063982e-01\n",
      "   -1.36030242e-01]\n",
      " [ -2.26206556e-01  -1.95534192e-02  -2.23962471e-01  -1.12002432e-01\n",
      "    1.08533703e-01]\n",
      " [  2.17899978e-02   2.81950563e-01   1.09449150e-02  -1.12312943e-01\n",
      "   -5.40933423e-02]\n",
      " [ -3.98476273e-02   2.04740912e-01   1.59185305e-01  -6.73054308e-02\n",
      "   -1.87974036e-01]\n",
      " [  2.80931778e-02  -1.61159232e-01  -1.36441812e-01   2.24786729e-01\n",
      "   -1.18945669e-02]\n",
      " [ -3.02630723e-01   5.76886721e-02   3.58637376e-03  -7.43813589e-02\n",
      "   -1.54902130e-01]\n",
      " [ -1.17911380e-02  -7.23512005e-03  -1.93502933e-01  -2.91571051e-01\n",
      "   -6.17859792e-03]\n",
      " [  3.17648835e-02   1.79958940e-01  -3.20607185e-01  -1.16240770e-01\n",
      "   -6.72343895e-02]\n",
      " [  1.09323241e-01   9.32811499e-02  -1.85489431e-01  -6.37329593e-02\n",
      "    8.74035805e-02]\n",
      " [  3.98312323e-03   2.77948827e-02   6.96689710e-02   8.42459053e-02\n",
      "    6.69312403e-02]\n",
      " [  2.12946266e-01  -2.32638553e-01  -1.02717914e-01  -1.76433325e-01\n",
      "    1.08540289e-01]\n",
      " [  1.73381537e-01  -1.85420364e-01   6.88029546e-03  -1.95092395e-01\n",
      "   -1.46013293e-02]\n",
      " [ -1.05008461e-01  -1.35743573e-01   1.50449470e-01  -6.81110099e-02\n",
      "    2.20765159e-01]\n",
      " [  1.34279057e-01  -1.24424472e-01   1.60360768e-01   9.22161862e-02\n",
      "   -9.96368751e-02]\n",
      " [  7.69823836e-03   1.42576247e-01  -2.25998357e-01   2.37119347e-02\n",
      "   -7.57799372e-02]\n",
      " [  5.49867190e-02   4.17790785e-02  -1.90580189e-01   8.47207829e-02\n",
      "   -8.20112750e-02]\n",
      " [  1.34608418e-01   1.76150173e-01   4.78190444e-02  -3.13335896e-01\n",
      "   -1.06882311e-01]\n",
      " [  1.88396666e-02   1.28146723e-01  -1.14791118e-01  -2.83091236e-02\n",
      "   -3.67954560e-02]\n",
      " [  8.73533338e-02   1.80627014e-02   8.33760668e-03   2.11010367e-01\n",
      "    9.16196182e-02]\n",
      " [ -8.28247890e-02   3.84903327e-02  -1.36359453e-01  -2.85690039e-01\n",
      "    1.74023420e-01]\n",
      " [ -1.40651181e-01   6.18501268e-02   2.68409640e-01  -1.60589591e-01\n",
      "   -2.00768054e-01]\n",
      " [  3.63403037e-02   2.05893759e-02   3.96665595e-02  -1.62851587e-01\n",
      "   -2.70657957e-01]\n",
      " [  2.68978029e-01   2.35285565e-01  -1.46127567e-02  -4.15724888e-03\n",
      "    1.21943720e-01]\n",
      " [  1.34755269e-01   1.73789501e-01   1.89190991e-02  -2.23914504e-01\n",
      "   -1.82571396e-01]\n",
      " [  1.61165837e-02   1.16807722e-01   1.70109645e-02  -1.40249869e-02\n",
      "    7.73110092e-02]\n",
      " [  1.77166745e-01  -6.22640997e-02  -5.17855138e-02  -3.30070332e-02\n",
      "    4.28905301e-02]\n",
      " [  1.39905676e-01   8.70841891e-02  -8.58858749e-02  -1.39169216e-01\n",
      "    1.29885729e-02]\n",
      " [  6.01510890e-02  -2.67800480e-01   1.99689388e-01  -2.73203164e-01\n",
      "    8.89929160e-02]\n",
      " [ -6.44790679e-02  -8.45821947e-02   1.30007893e-01  -3.61163244e-02\n",
      "   -4.99871224e-02]\n",
      " [ -1.29241914e-01  -8.90789926e-02  -1.70708105e-01  -4.38946821e-02\n",
      "    1.87144458e-01]\n",
      " [ -5.11401938e-03  -7.27466643e-02  -2.66160488e-01   2.49253865e-03\n",
      "   -1.20512664e-01]\n",
      " [ -1.87740535e-01  -1.99432716e-01  -9.14473981e-02   7.99560249e-02\n",
      "   -2.16471255e-01]\n",
      " [ -4.98753563e-02   1.45181850e-01  -8.03309307e-02   5.88191822e-02\n",
      "   -7.76891112e-02]\n",
      " [ -2.24141970e-01   4.78630029e-02   6.13882067e-03  -1.29164428e-01\n",
      "    1.37223780e-01]\n",
      " [  1.92593426e-01  -2.59133369e-01  -1.58049598e-01  -1.83170289e-01\n",
      "   -1.52721405e-01]\n",
      " [ -1.43922016e-01   8.35925937e-02  -5.99112324e-02   1.43935874e-01\n",
      "    2.10552469e-01]\n",
      " [ -5.24504073e-02  -2.32628770e-02  -1.88757047e-01  -4.71291393e-02\n",
      "    7.53354877e-02]\n",
      " [  4.68228087e-02  -6.70496821e-02   2.25173220e-01   3.20915818e-01\n",
      "    4.04942445e-02]\n",
      " [ -7.74380714e-02   1.36427179e-01   1.86275050e-01   1.46138491e-02\n",
      "   -5.86189739e-02]\n",
      " [  2.61502206e-01  -1.23168491e-01   4.10405770e-02   9.71802250e-02\n",
      "   -1.46030858e-01]\n",
      " [ -1.86712310e-01   3.84249836e-02  -5.47481701e-02   9.68295038e-02\n",
      "   -8.98239240e-02]\n",
      " [  6.64784908e-02  -7.31839463e-02   4.30199988e-02  -2.80914009e-01\n",
      "    1.75866634e-01]\n",
      " [ -2.52673943e-02  -9.53710377e-02  -7.87159428e-02   2.82348841e-01\n",
      "    1.74729094e-01]\n",
      " [  1.29281074e-01  -2.06503168e-01   5.08791171e-02  -1.61172852e-01\n",
      "   -1.34705201e-01]\n",
      " [ -1.40838534e-01  -3.98279652e-02  -1.38026431e-01  -1.78234354e-01\n",
      "    2.58497626e-01]\n",
      " [  3.67548130e-02   9.51290578e-02  -4.24230471e-03   2.81148069e-02\n",
      "    5.86838648e-02]\n",
      " [  2.19140664e-01  -1.51676983e-01   1.56634957e-01  -1.19046889e-01\n",
      "   -7.08343908e-02]\n",
      " [  7.10062757e-02  -2.14864507e-01   2.73327418e-02   1.38866737e-01\n",
      "   -2.62923241e-01]\n",
      " [ -1.91086516e-01  -9.49386060e-02  -2.43432373e-02  -2.67758697e-01\n",
      "   -1.19096078e-01]\n",
      " [ -1.84490010e-01  -1.58176616e-01  -1.97568140e-03   5.27584255e-02\n",
      "   -3.96606065e-02]\n",
      " [ -1.07087558e-02  -4.44282144e-02  -1.98434487e-01  -1.06984995e-01\n",
      "   -3.45852971e-02]\n",
      " [ -1.14794828e-01  -3.01010758e-02  -2.68419236e-02   3.00478730e-02\n",
      "    3.21203582e-02]\n",
      " [  1.99824765e-01  -2.90358383e-02  -6.53321669e-02  -2.14237303e-01\n",
      "    9.72766206e-02]\n",
      " [ -1.99994057e-01  -1.64291039e-01   2.43611395e-01  -2.26733893e-01\n",
      "   -1.12931421e-02]\n",
      " [  6.67264163e-02  -1.84214458e-01  -8.04885030e-02  -2.00619057e-01\n",
      "    8.54431540e-02]\n",
      " [  2.80932579e-02   4.33001630e-02   1.56439766e-01  -2.54283585e-02\n",
      "    2.01745942e-01]\n",
      " [  9.98268798e-02   1.72213092e-01  -4.69656065e-02  -4.97147329e-02\n",
      "    6.23334870e-02]\n",
      " [ -1.62885830e-01  -1.05922244e-01  -2.52431780e-01   1.91418156e-01\n",
      "   -1.29438445e-01]\n",
      " [  1.90729678e-01  -2.58838348e-02  -4.32900116e-02  -2.56282091e-01\n",
      "   -1.44517899e-01]\n",
      " [ -9.68867242e-02   1.14944555e-01   1.10814199e-01  -6.39123172e-02\n",
      "   -3.58856414e-05]\n",
      " [  9.29681119e-03  -1.88829944e-01   4.64419164e-02  -1.09916747e-01\n",
      "    7.89356828e-02]\n",
      " [ -8.46171752e-02   3.95876393e-02  -1.40860841e-01  -2.12245621e-02\n",
      "    1.35500357e-01]\n",
      " [  1.11968122e-01  -1.09180488e-01  -1.01108916e-01   1.47760779e-01\n",
      "    1.57618701e-01]\n",
      " [ -1.45524934e-01   7.73568079e-02  -1.10501751e-01  -1.10057786e-01\n",
      "   -8.84440839e-02]\n",
      " [ -9.71044004e-02   1.07315136e-03   1.40449941e-01   2.61139926e-02\n",
      "    4.93671298e-02]\n",
      " [  2.20308885e-01   1.00488484e-01   9.17493850e-02  -1.57160163e-01\n",
      "    1.49913579e-02]\n",
      " [  1.30739108e-01   5.92073835e-02   1.71948224e-01   6.18037917e-02\n",
      "    1.74798071e-01]\n",
      " [ -2.47836739e-01   1.60195410e-01   4.35812771e-02   1.51605904e-01\n",
      "   -7.42484480e-02]\n",
      " [ -2.73580011e-02  -1.87478453e-01  -1.50677100e-01   3.08850408e-01\n",
      "    1.52810216e-01]\n",
      " [  1.41037285e-01   5.57719544e-02  -1.59059837e-02  -4.02703993e-02\n",
      "   -3.06972086e-01]\n",
      " [ -7.39031658e-02   7.00697824e-02   1.36986539e-01  -1.09762466e-02\n",
      "   -2.60643840e-01]\n",
      " [  5.29645616e-03   5.58492728e-02   1.08188577e-01   4.07461599e-02\n",
      "    9.29882303e-02]\n",
      " [ -2.68210530e-01   1.02615952e-01   7.64468610e-02   9.64709148e-02\n",
      "   -4.63861190e-02]\n",
      " [  1.47411764e-01  -2.00276226e-02   1.27047077e-01  -2.98273116e-02\n",
      "   -2.34689027e-01]\n",
      " [  8.34999532e-02   1.52617931e-01  -1.99618489e-01   2.86219686e-01\n",
      "    1.49482802e-01]\n",
      " [ -3.05000506e-03  -1.06639616e-01   3.43486071e-02  -1.17106467e-01\n",
      "   -1.34440079e-01]\n",
      " [ -1.84726790e-01   1.01653695e-01   3.30243446e-03   2.74779499e-02\n",
      "   -7.77080730e-02]\n",
      " [ -9.47085097e-02  -2.02692971e-02  -2.03018799e-01   6.43731728e-02\n",
      "    1.33884335e-02]\n",
      " [ -2.34766142e-03   1.31283641e-01   7.43637532e-02   1.80283979e-01\n",
      "    1.59741476e-01]\n",
      " [ -1.52866229e-01  -2.25400329e-01  -1.21165544e-01  -6.14393614e-02\n",
      "    7.31659308e-02]\n",
      " [ -4.19547930e-02  -1.50323519e-02  -2.36947052e-02   2.00812176e-01\n",
      "   -5.37914596e-02]\n",
      " [  1.40593469e-01   1.42893791e-01   1.25329435e-01  -4.56808470e-02\n",
      "    1.62847620e-02]\n",
      " [ -3.23912501e-02   2.62192070e-01   9.06310603e-02   8.91588852e-02\n",
      "   -6.53658882e-02]\n",
      " [  3.96075100e-02  -5.92167601e-02  -1.60843298e-01   2.36444786e-01\n",
      "    2.19690710e-01]\n",
      " [  2.61300772e-01  -7.92502984e-02  -9.33492780e-02   1.66102890e-02\n",
      "   -7.28795752e-02]\n",
      " [  1.03565648e-01  -6.83662109e-03  -8.10674205e-02   2.77023375e-01\n",
      "    2.24683457e-03]\n",
      " [  9.50056463e-02  -2.87368655e-01   7.81885907e-02   3.37718092e-02\n",
      "   -1.28165424e-01]\n",
      " [ -2.30900967e-03  -2.97600955e-01  -4.99555729e-02  -2.03251205e-02\n",
      "   -1.00860432e-01]\n",
      " [  1.17425926e-01   3.71715240e-02  -4.15655337e-02   1.59981534e-01\n",
      "    8.90316218e-02]\n",
      " [ -9.24652517e-02  -1.72934040e-01   9.11903083e-02  -1.80131286e-01\n",
      "    7.86150526e-03]\n",
      " [ -1.23263046e-01  -8.51790141e-03   1.86095405e-02   2.31902227e-02\n",
      "   -5.43810315e-02]\n",
      " [ -6.39759749e-02  -1.37387037e-01  -3.81988622e-02  -4.24212962e-02\n",
      "    1.14002246e-02]\n",
      " [ -1.45267934e-01  -2.10079700e-02   2.58387297e-01  -8.12230557e-02\n",
      "   -6.98542073e-02]\n",
      " [  1.04345866e-01  -2.24862527e-02   1.25588514e-02  -1.35831624e-01\n",
      "    1.79227918e-01]\n",
      " [  3.64418663e-02   1.58072367e-01   1.61619149e-02   2.40701780e-01\n",
      "    1.62675351e-01]\n",
      " [ -1.20781146e-01  -8.02970752e-02   5.38276471e-02  -8.13703388e-02\n",
      "   -9.93915796e-02]\n",
      " [ -1.07511558e-01   1.58642709e-01   7.40583288e-03  -8.73893574e-02\n",
      "   -1.81689173e-01]\n",
      " [ -1.22985020e-01  -1.59469679e-01   1.08357154e-01  -2.16674671e-01\n",
      "   -1.73142672e-01]\n",
      " [ -7.93487206e-02  -2.63637125e-01  -5.85182458e-02   2.93511525e-02\n",
      "   -1.37407601e-01]\n",
      " [ -7.91469514e-02   6.15606904e-02  -9.12913680e-02   8.45946148e-02\n",
      "   -7.45240450e-02]\n",
      " [ -2.11291686e-01  -1.20506562e-01   6.92059696e-02  -9.88722686e-03\n",
      "    8.56562182e-02]\n",
      " [ -1.22582935e-01  -2.58019745e-01   2.71427512e-01  -1.10058278e-01\n",
      "    3.36405300e-02]\n",
      " [ -5.40525801e-02  -2.01847225e-01   3.88451596e-03  -5.73709123e-02\n",
      "   -5.96416369e-02]\n",
      " [ -2.38847077e-01   6.68130443e-02  -9.16510224e-02   1.48448393e-01\n",
      "   -2.03222036e-02]\n",
      " [ -5.27827255e-02   4.38802829e-03   9.99540091e-03  -1.84405744e-02\n",
      "    1.41133353e-01]\n",
      " [ -7.33837187e-02   1.25994593e-01   3.93473879e-02  -1.24112435e-01\n",
      "   -9.60230455e-02]\n",
      " [  2.13242210e-02  -9.53736752e-02  -1.67107254e-01  -5.10896705e-02\n",
      "   -3.28244753e-02]\n",
      " [  2.44071871e-01  -1.45322144e-01  -2.95591563e-01  -2.56898135e-01\n",
      "    2.50754356e-01]\n",
      " [  2.09780082e-01   1.05572686e-01  -9.43009034e-02  -4.36056852e-02\n",
      "    3.98891699e-03]\n",
      " [  2.15917274e-01   2.67460058e-03  -1.28958732e-01  -1.03396941e-02\n",
      "    5.81448805e-03]\n",
      " [  4.21004891e-02  -9.32503864e-02  -2.00607091e-01  -2.01905340e-01\n",
      "    3.46637368e-02]\n",
      " [ -1.75529942e-01   8.28921571e-02   2.06849903e-01  -1.63655486e-02\n",
      "    2.89917737e-01]\n",
      " [  2.06837982e-01  -4.29155603e-02   1.15129240e-01  -1.09891281e-01\n",
      "    4.18895036e-02]\n",
      " [  1.58171132e-01   2.43919507e-01   1.00063756e-01  -2.32162029e-01\n",
      "   -1.31614944e-02]\n",
      " [  1.66320682e-01  -2.41071165e-01  -6.80427030e-02   1.55056611e-01\n",
      "   -1.91188321e-01]\n",
      " [  1.00466646e-01  -7.25628063e-02  -1.40239403e-01  -7.06359297e-02\n",
      "   -1.15034945e-01]\n",
      " [  1.99784204e-01  -6.40502274e-02   4.95138504e-02  -1.10707395e-01\n",
      "    6.21632263e-02]\n",
      " [ -2.03156292e-01   1.00742869e-01  -1.42849572e-02  -8.19547195e-03\n",
      "   -3.39890510e-01]\n",
      " [ -1.69722468e-03  -1.91138595e-01  -1.15661882e-02   1.77162915e-01\n",
      "    2.02251188e-02]\n",
      " [  6.20474368e-02   1.56820238e-01   4.64294516e-02  -1.46643326e-01\n",
      "   -1.71315342e-01]\n",
      " [ -2.14609072e-01  -5.62263578e-02   2.11248100e-02   1.17799565e-01\n",
      "   -9.67374817e-02]\n",
      " [ -2.31602397e-02   2.98859179e-01  -1.25565052e-01  -1.02357566e-01\n",
      "    1.77720878e-02]\n",
      " [ -2.40302961e-02   2.77252961e-02  -1.29349709e-01   4.49255221e-02\n",
      "   -2.77544148e-02]\n",
      " [  1.25897974e-02  -1.37195036e-01  -1.92523599e-02  -5.77223748e-02\n",
      "   -1.49352819e-01]\n",
      " [ -9.71998721e-02   1.92999188e-02  -1.98334053e-01   1.53375670e-01\n",
      "    2.11577699e-01]\n",
      " [ -1.24687351e-01  -1.96019098e-01   9.37647969e-02  -7.65662193e-02\n",
      "   -6.43666834e-02]\n",
      " [  8.09687655e-03   9.08691511e-02   3.19265760e-02   1.12690903e-01\n",
      "    1.72431141e-01]\n",
      " [  1.02720335e-01   2.27913372e-02  -1.16927736e-01  -2.06150547e-01\n",
      "   -7.64667690e-02]]\n",
      "Softmax_Layer/bias:0 with value [-0.00880131 -0.00159127  0.00683142  0.00266058 -0.00240504]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    restore_saver = tf.train.import_meta_graph(\"./Models/yen/mnist_5.ckpt.meta\")\n",
    "    restore_saver.restore(sess, tf.train.latest_checkpoint('./Models/yen/'))\n",
    "#     sess.run(tf.global_variables_initializer()) # initialize the weights \n",
    "#     sess.run(tf.local_variables_initializer()) # initialize the local variables hidden in the tf.metrics.recallmethod.\n",
    "\n",
    "    all_vars = tf.trainable_variables()\n",
    "    for v in all_vars:\n",
    "\n",
    "        print(\"%s with value %s\" % (v.name, sess.run(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Models/yen/mnist_5.ckpt\n",
      "   1 epochs,   Validation loss:1.52103,   Best loss:1.52103,   Accuracy:36.67%/ 41.56%\n",
      "   2 epochs,   Validation loss:1.51702,   Best loss:1.51702,   Accuracy:37.33%/ 42.05%\n",
      "   3 epochs,   Validation loss:1.51312,   Best loss:1.51312,   Accuracy:38.00%/ 42.42%\n",
      "   4 epochs,   Validation loss:1.50918,   Best loss:1.50918,   Accuracy:39.33%/ 42.97%\n",
      "   5 epochs,   Validation loss:1.50506,   Best loss:1.50506,   Accuracy:39.33%/ 43.16%\n",
      "   6 epochs,   Validation loss:1.50065,   Best loss:1.50065,   Accuracy:38.67%/ 43.43%\n",
      "   7 epochs,   Validation loss:1.49589,   Best loss:1.49589,   Accuracy:39.33%/ 43.65%\n",
      "   8 epochs,   Validation loss:1.49083,   Best loss:1.49083,   Accuracy:40.00%/ 43.92%\n",
      "   9 epochs,   Validation loss:1.48557,   Best loss:1.48557,   Accuracy:40.67%/ 44.15%\n",
      "  10 epochs,   Validation loss:1.48024,   Best loss:1.48024,   Accuracy:41.33%/ 44.44%\n",
      "  11 epochs,   Validation loss:1.47491,   Best loss:1.47491,   Accuracy:41.33%/ 44.79%\n",
      "  12 epochs,   Validation loss:1.46962,   Best loss:1.46962,   Accuracy:42.67%/ 45.13%\n",
      "  13 epochs,   Validation loss:1.46442,   Best loss:1.46442,   Accuracy:43.33%/ 45.53%\n",
      "  14 epochs,   Validation loss:1.45935,   Best loss:1.45935,   Accuracy:44.00%/ 45.77%\n",
      "  15 epochs,   Validation loss:1.45447,   Best loss:1.45447,   Accuracy:44.67%/ 45.94%\n",
      "  16 epochs,   Validation loss:1.44984,   Best loss:1.44984,   Accuracy:44.67%/ 46.25%\n",
      "  17 epochs,   Validation loss:1.44553,   Best loss:1.44553,   Accuracy:44.67%/ 46.64%\n",
      "  18 epochs,   Validation loss:1.44153,   Best loss:1.44153,   Accuracy:44.67%/ 46.68%\n",
      "  19 epochs,   Validation loss:1.43786,   Best loss:1.43786,   Accuracy:46.00%/ 47.13%\n",
      "  20 epochs,   Validation loss:1.43447,   Best loss:1.43447,   Accuracy:46.00%/ 47.38%\n",
      "  21 epochs,   Validation loss:1.43135,   Best loss:1.43135,   Accuracy:46.00%/ 47.52%\n",
      "  22 epochs,   Validation loss:1.42848,   Best loss:1.42848,   Accuracy:46.00%/ 47.58%\n",
      "  23 epochs,   Validation loss:1.42585,   Best loss:1.42585,   Accuracy:46.67%/ 47.64%\n",
      "  24 epochs,   Validation loss:1.42345,   Best loss:1.42345,   Accuracy:47.33%/ 47.83%\n",
      "  25 epochs,   Validation loss:1.42126,   Best loss:1.42126,   Accuracy:48.00%/ 47.99%\n",
      "  26 epochs,   Validation loss:1.41924,   Best loss:1.41924,   Accuracy:48.00%/ 48.04%\n",
      "  27 epochs,   Validation loss:1.41735,   Best loss:1.41735,   Accuracy:48.00%/ 48.24%\n",
      "  28 epochs,   Validation loss:1.41553,   Best loss:1.41553,   Accuracy:48.00%/ 48.41%\n",
      "  29 epochs,   Validation loss:1.41374,   Best loss:1.41374,   Accuracy:48.00%/ 48.55%\n",
      "  30 epochs,   Validation loss:1.41190,   Best loss:1.41190,   Accuracy:48.67%/ 48.67%\n",
      "  31 epochs,   Validation loss:1.40996,   Best loss:1.40996,   Accuracy:48.67%/ 48.82%\n",
      "  32 epochs,   Validation loss:1.40782,   Best loss:1.40782,   Accuracy:48.67%/ 49.04%\n",
      "  33 epochs,   Validation loss:1.40535,   Best loss:1.40535,   Accuracy:48.67%/ 49.25%\n",
      "  34 epochs,   Validation loss:1.40240,   Best loss:1.40240,   Accuracy:48.67%/ 49.48%\n",
      "  35 epochs,   Validation loss:1.39883,   Best loss:1.39883,   Accuracy:49.33%/ 49.58%\n",
      "  36 epochs,   Validation loss:1.39451,   Best loss:1.39451,   Accuracy:49.33%/ 49.83%\n",
      "  37 epochs,   Validation loss:1.38947,   Best loss:1.38947,   Accuracy:50.00%/ 49.87%\n",
      "  38 epochs,   Validation loss:1.38390,   Best loss:1.38390,   Accuracy:50.00%/ 50.01%\n",
      "  39 epochs,   Validation loss:1.37816,   Best loss:1.37816,   Accuracy:52.67%/ 50.20%\n",
      "  40 epochs,   Validation loss:1.37257,   Best loss:1.37257,   Accuracy:53.33%/ 50.36%\n",
      "  41 epochs,   Validation loss:1.36725,   Best loss:1.36725,   Accuracy:53.33%/ 50.73%\n",
      "  42 epochs,   Validation loss:1.36213,   Best loss:1.36213,   Accuracy:53.33%/ 51.08%\n",
      "  43 epochs,   Validation loss:1.35716,   Best loss:1.35716,   Accuracy:54.00%/ 51.39%\n",
      "  44 epochs,   Validation loss:1.35249,   Best loss:1.35249,   Accuracy:54.67%/ 51.90%\n",
      "  45 epochs,   Validation loss:1.34834,   Best loss:1.34834,   Accuracy:55.33%/ 52.33%\n",
      "  46 epochs,   Validation loss:1.34467,   Best loss:1.34467,   Accuracy:55.33%/ 52.68%\n",
      "  47 epochs,   Validation loss:1.34107,   Best loss:1.34107,   Accuracy:55.33%/ 53.03%\n",
      "  48 epochs,   Validation loss:1.33703,   Best loss:1.33703,   Accuracy:55.33%/ 53.43%\n",
      "  49 epochs,   Validation loss:1.33235,   Best loss:1.33235,   Accuracy:56.67%/ 53.77%\n",
      "  50 epochs,   Validation loss:1.32725,   Best loss:1.32725,   Accuracy:57.33%/ 54.29%\n",
      "  51 epochs,   Validation loss:1.32188,   Best loss:1.32188,   Accuracy:58.00%/ 54.82%\n",
      "  52 epochs,   Validation loss:1.31664,   Best loss:1.31664,   Accuracy:59.33%/ 55.21%\n",
      "  53 epochs,   Validation loss:1.31222,   Best loss:1.31222,   Accuracy:59.33%/ 55.50%\n",
      "  54 epochs,   Validation loss:1.30847,   Best loss:1.30847,   Accuracy:58.67%/ 56.14%\n",
      "  55 epochs,   Validation loss:1.30486,   Best loss:1.30486,   Accuracy:59.33%/ 56.72%\n",
      "  56 epochs,   Validation loss:1.30109,   Best loss:1.30109,   Accuracy:59.33%/ 57.21%\n",
      "  57 epochs,   Validation loss:1.29697,   Best loss:1.29697,   Accuracy:58.67%/ 57.56%\n",
      "  58 epochs,   Validation loss:1.29231,   Best loss:1.29231,   Accuracy:59.33%/ 58.22%\n",
      "  59 epochs,   Validation loss:1.28676,   Best loss:1.28676,   Accuracy:60.00%/ 58.73%\n",
      "  60 epochs,   Validation loss:1.27995,   Best loss:1.27995,   Accuracy:61.33%/ 59.35%\n",
      "  61 epochs,   Validation loss:1.27175,   Best loss:1.27175,   Accuracy:63.33%/ 60.03%\n",
      "  62 epochs,   Validation loss:1.26271,   Best loss:1.26271,   Accuracy:64.67%/ 60.56%\n",
      "  63 epochs,   Validation loss:1.25406,   Best loss:1.25406,   Accuracy:65.33%/ 61.00%\n",
      "  64 epochs,   Validation loss:1.24684,   Best loss:1.24684,   Accuracy:66.67%/ 61.65%\n",
      "  65 epochs,   Validation loss:1.24116,   Best loss:1.24116,   Accuracy:66.67%/ 62.13%\n",
      "  66 epochs,   Validation loss:1.23651,   Best loss:1.23651,   Accuracy:66.67%/ 62.62%\n",
      "  67 epochs,   Validation loss:1.23249,   Best loss:1.23249,   Accuracy:67.33%/ 63.16%\n",
      "  68 epochs,   Validation loss:1.22895,   Best loss:1.22895,   Accuracy:68.00%/ 63.65%\n",
      "  69 epochs,   Validation loss:1.22587,   Best loss:1.22587,   Accuracy:68.67%/ 63.81%\n",
      "  70 epochs,   Validation loss:1.22322,   Best loss:1.22322,   Accuracy:68.67%/ 64.18%\n",
      "  71 epochs,   Validation loss:1.22097,   Best loss:1.22097,   Accuracy:68.67%/ 64.64%\n",
      "  72 epochs,   Validation loss:1.21904,   Best loss:1.21904,   Accuracy:68.67%/ 64.97%\n",
      "  73 epochs,   Validation loss:1.21735,   Best loss:1.21735,   Accuracy:69.33%/ 65.46%\n",
      "  74 epochs,   Validation loss:1.21584,   Best loss:1.21584,   Accuracy:70.00%/ 65.77%\n",
      "  75 epochs,   Validation loss:1.21450,   Best loss:1.21450,   Accuracy:70.00%/ 66.12%\n",
      "  76 epochs,   Validation loss:1.21343,   Best loss:1.21343,   Accuracy:71.33%/ 66.61%\n",
      "  77 epochs,   Validation loss:1.21273,   Best loss:1.21273,   Accuracy:71.33%/ 66.84%\n",
      "  78 epochs,   Validation loss:1.21239,   Best loss:1.21239,   Accuracy:70.67%/ 67.02%\n",
      "  79 epochs,   Validation loss:1.21227,   Best loss:1.21227,   Accuracy:70.67%/ 67.15%\n",
      "  80 epochs,   Validation loss:1.21224,   Best loss:1.21224,   Accuracy:70.67%/ 67.31%\n",
      "  81 epochs,   Validation loss:1.21218,   Best loss:1.21218,   Accuracy:70.67%/ 67.62%\n",
      "  82 epochs,   Validation loss:1.21200,   Best loss:1.21200,   Accuracy:70.67%/ 67.87%\n",
      "  83 epochs,   Validation loss:1.21165,   Best loss:1.21165,   Accuracy:70.67%/ 67.95%\n",
      "  84 epochs,   Validation loss:1.21112,   Best loss:1.21112,   Accuracy:70.67%/ 68.11%\n",
      "  85 epochs,   Validation loss:1.21041,   Best loss:1.21041,   Accuracy:70.67%/ 68.13%\n",
      "  86 epochs,   Validation loss:1.20953,   Best loss:1.20953,   Accuracy:70.67%/ 68.26%\n",
      "  87 epochs,   Validation loss:1.20849,   Best loss:1.20849,   Accuracy:71.33%/ 68.30%\n",
      "  88 epochs,   Validation loss:1.20733,   Best loss:1.20733,   Accuracy:71.33%/ 68.44%\n",
      "  89 epochs,   Validation loss:1.20605,   Best loss:1.20605,   Accuracy:71.33%/ 68.50%\n",
      "  90 epochs,   Validation loss:1.20469,   Best loss:1.20469,   Accuracy:71.33%/ 68.63%\n",
      "  91 epochs,   Validation loss:1.20325,   Best loss:1.20325,   Accuracy:70.67%/ 68.59%\n",
      "  92 epochs,   Validation loss:1.20175,   Best loss:1.20175,   Accuracy:70.67%/ 68.67%\n",
      "  93 epochs,   Validation loss:1.20021,   Best loss:1.20021,   Accuracy:70.67%/ 68.71%\n",
      "  94 epochs,   Validation loss:1.19866,   Best loss:1.19866,   Accuracy:70.67%/ 68.71%\n",
      "  95 epochs,   Validation loss:1.19713,   Best loss:1.19713,   Accuracy:70.67%/ 68.90%\n",
      "  96 epochs,   Validation loss:1.19564,   Best loss:1.19564,   Accuracy:70.67%/ 69.08%\n",
      "  97 epochs,   Validation loss:1.19423,   Best loss:1.19423,   Accuracy:72.00%/ 69.31%\n",
      "  98 epochs,   Validation loss:1.19291,   Best loss:1.19291,   Accuracy:72.00%/ 69.37%\n",
      "  99 epochs,   Validation loss:1.19169,   Best loss:1.19169,   Accuracy:72.00%/ 69.47%\n",
      " 100 epochs,   Validation loss:1.19056,   Best loss:1.19056,   Accuracy:73.33%/ 69.51%\n",
      " 101 epochs,   Validation loss:1.18951,   Best loss:1.18951,   Accuracy:73.33%/ 69.59%\n",
      " 102 epochs,   Validation loss:1.18849,   Best loss:1.18849,   Accuracy:74.00%/ 69.80%\n",
      " 103 epochs,   Validation loss:1.18747,   Best loss:1.18747,   Accuracy:74.00%/ 69.90%\n",
      " 104 epochs,   Validation loss:1.18643,   Best loss:1.18643,   Accuracy:74.00%/ 69.97%\n",
      " 105 epochs,   Validation loss:1.18534,   Best loss:1.18534,   Accuracy:74.00%/ 70.09%\n",
      " 106 epochs,   Validation loss:1.18418,   Best loss:1.18418,   Accuracy:74.00%/ 70.25%\n",
      " 107 epochs,   Validation loss:1.18297,   Best loss:1.18297,   Accuracy:74.00%/ 70.36%\n",
      " 108 epochs,   Validation loss:1.18172,   Best loss:1.18172,   Accuracy:74.00%/ 70.54%\n",
      " 109 epochs,   Validation loss:1.18047,   Best loss:1.18047,   Accuracy:74.00%/ 70.71%\n",
      " 110 epochs,   Validation loss:1.17924,   Best loss:1.17924,   Accuracy:74.67%/ 70.77%\n",
      " 111 epochs,   Validation loss:1.17805,   Best loss:1.17805,   Accuracy:74.67%/ 70.91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 112 epochs,   Validation loss:1.17693,   Best loss:1.17693,   Accuracy:74.67%/ 71.10%\n",
      " 113 epochs,   Validation loss:1.17589,   Best loss:1.17589,   Accuracy:74.67%/ 71.28%\n",
      " 114 epochs,   Validation loss:1.17494,   Best loss:1.17494,   Accuracy:74.67%/ 71.30%\n",
      " 115 epochs,   Validation loss:1.17407,   Best loss:1.17407,   Accuracy:74.67%/ 71.47%\n",
      " 116 epochs,   Validation loss:1.17328,   Best loss:1.17328,   Accuracy:74.67%/ 71.61%\n",
      " 117 epochs,   Validation loss:1.17256,   Best loss:1.17256,   Accuracy:74.67%/ 71.67%\n",
      " 118 epochs,   Validation loss:1.17189,   Best loss:1.17189,   Accuracy:74.67%/ 71.86%\n",
      " 119 epochs,   Validation loss:1.17126,   Best loss:1.17126,   Accuracy:74.67%/ 71.92%\n",
      " 120 epochs,   Validation loss:1.17064,   Best loss:1.17064,   Accuracy:74.00%/ 71.94%\n",
      " 121 epochs,   Validation loss:1.17002,   Best loss:1.17002,   Accuracy:74.00%/ 72.00%\n",
      " 122 epochs,   Validation loss:1.16937,   Best loss:1.16937,   Accuracy:74.00%/ 72.17%\n",
      " 123 epochs,   Validation loss:1.16865,   Best loss:1.16865,   Accuracy:74.00%/ 72.25%\n",
      " 124 epochs,   Validation loss:1.16786,   Best loss:1.16786,   Accuracy:74.00%/ 72.37%\n",
      " 125 epochs,   Validation loss:1.16698,   Best loss:1.16698,   Accuracy:74.00%/ 72.43%\n",
      " 126 epochs,   Validation loss:1.16598,   Best loss:1.16598,   Accuracy:74.00%/ 72.58%\n",
      " 127 epochs,   Validation loss:1.16488,   Best loss:1.16488,   Accuracy:74.00%/ 72.64%\n",
      " 128 epochs,   Validation loss:1.16367,   Best loss:1.16367,   Accuracy:74.00%/ 72.78%\n",
      " 129 epochs,   Validation loss:1.16236,   Best loss:1.16236,   Accuracy:74.67%/ 72.99%\n",
      "\n",
      "Training Time:0.829563\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "no_update_tolerant = 20\n",
    "\n",
    "no_update_progress = 0\n",
    "current_best_accuracy = 0.0\n",
    "current_best_loss = 10.0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    restore_saver = tf.train.import_meta_graph(\"./Models/yen/mnist_5.ckpt.meta\") # load meta graph\n",
    "    restore_saver.restore(sess, tf.train.latest_checkpoint('./Models/yen/')) # restore weights\n",
    "#     sess.run(tf.global_variables_initializer()) # initialize the weights \n",
    "#     sess.run(tf.local_variables_initializer()) # initialize the local variables hidden in the tf.metrics.recallmethod.\n",
    "\n",
    "    \"\"\"\n",
    "    Graph Define Step\n",
    "    \n",
    "    1. Restore the variable about the component in graph which help to retrieve and assign value\n",
    "    \"\"\"\n",
    "    graph = tf.get_default_graph() # get original graph\n",
    "    X = graph.get_tensor_by_name(\"Input_X:0\") # assign placeholder variables with input X\n",
    "    y = graph.get_tensor_by_name(\"Input_Label:0\")# assign placeholder variables with input Label\n",
    "#     mode = graph.get_tensor_by_name(\"Mode:0\")# assign placeholder variables with input Mode\n",
    "    loss = graph.get_tensor_by_name(\"Loss:0\") # assign variables about loss\n",
    "    Y_proba = graph.get_tensor_by_name(\"Softmax_Layer/Softmax:0\") # assign variables about loss\n",
    "    accuracy = graph.get_tensor_by_name(\"Accuracy:0\") # assign variables  about accuracy\n",
    "#     optimizer = graph.get_operation_by_name(\"Adam\")# assign variables about optimizer which is adam optimizer\n",
    "\n",
    "#     training_op = graph.get_collection('training_op')[0]\n",
    "#     print(type(optimizer))\n",
    "#     print(Y_proba.op.inputs[0])\n",
    "    \"\"\"\n",
    "    2. Get the softmax layer which is what we would like to train\n",
    "    \"\"\"\n",
    "    output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Softmax_Layer\") # get softmax layer by scope name\n",
    "    \n",
    "    \"\"\"\n",
    "    3. Define the optimizer and assign to only update on softmax_layer which defined above\n",
    "    \"\"\"\n",
    "    optimizer = tf.train.AdamOptimizer(0.0017, name=\"Adam_Op\")\n",
    "    training_op = optimizer.minimize(loss, var_list=output_layer_vars) # minimize the loss and only update certern variables\n",
    "    var_list_init = tf.variables_initializer([optimizer.get_slot(var, name) for name in optimizer.get_slot_names() for var in output_layer_vars]+list(optimizer._get_beta_accumulators()))\n",
    "    sess.run(var_list_init)\n",
    "    \"\"\"\n",
    "    Training Step\n",
    "    \"\"\"\n",
    "    begin_time = time.time()\n",
    "    for epochs in range(1000): # training on 1000 epochs\n",
    "        _, train_loss = sess.run([training_op, loss], feed_dict={X:X_train2, y:y_train2}) # feed with training variables\n",
    "        valid_loss, valid_accuracy = sess.run([loss, accuracy], feed_dict={X:X_valid2, y:y_valid2}) # feed with validation variables\n",
    "        test_loss ,  test_accuracy = sess.run([loss, accuracy], feed_dict={X:X_test2, y:y_test2}) # feed with testing variables\n",
    "        \n",
    "        if valid_accuracy > current_best_accuracy: # if better\n",
    "            current_best_accuracy = valid_accuracy # keep record\n",
    "            no_update_progress = 0\n",
    "        else:                                      # if not better\n",
    "            no_update_progress += 1                # record on no update\n",
    "\n",
    "        if valid_loss < current_best_loss: # if better\n",
    "            current_best_loss = valid_loss # keep record            \n",
    "            \n",
    "        if no_update_progress == no_update_tolerant: break # stop training\n",
    "            \n",
    "                \n",
    "        print(\"{:4d} epochs,   Validation loss:{:.5f},   Best loss:{:.5f},   Accuracy:{:.2f}%/ {:.2f}%\".format(\n",
    "            epochs+1,\n",
    "            valid_loss,\n",
    "            current_best_loss,                                                                                       \n",
    "            valid_accuracy*100,\n",
    "            test_accuracy*100\n",
    "        ))\n",
    "    finish_itme = time.time()\n",
    "    print(\"\\nTraining Time:{:3f}\".format(finish_itme-begin_time))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Cache the Output of FIfth layer for Speed Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Models/yen/mnist_5.ckpt\n",
      "(500, 128)\n",
      "   1 epochs,   Validation loss:1.52103,   Best loss:1.52103,   Accuracy:36.67%/ 41.56%\n",
      "   2 epochs,   Validation loss:1.51702,   Best loss:1.51702,   Accuracy:37.33%/ 42.05%\n",
      "   3 epochs,   Validation loss:1.51312,   Best loss:1.51312,   Accuracy:38.00%/ 42.42%\n",
      "   4 epochs,   Validation loss:1.50918,   Best loss:1.50918,   Accuracy:39.33%/ 42.97%\n",
      "   5 epochs,   Validation loss:1.50506,   Best loss:1.50506,   Accuracy:39.33%/ 43.16%\n",
      "   6 epochs,   Validation loss:1.50065,   Best loss:1.50065,   Accuracy:38.67%/ 43.43%\n",
      "   7 epochs,   Validation loss:1.49589,   Best loss:1.49589,   Accuracy:39.33%/ 43.65%\n",
      "   8 epochs,   Validation loss:1.49083,   Best loss:1.49083,   Accuracy:40.00%/ 43.92%\n",
      "   9 epochs,   Validation loss:1.48557,   Best loss:1.48557,   Accuracy:40.67%/ 44.15%\n",
      "  10 epochs,   Validation loss:1.48024,   Best loss:1.48024,   Accuracy:41.33%/ 44.44%\n",
      "  11 epochs,   Validation loss:1.47491,   Best loss:1.47491,   Accuracy:41.33%/ 44.79%\n",
      "  12 epochs,   Validation loss:1.46962,   Best loss:1.46962,   Accuracy:42.67%/ 45.13%\n",
      "  13 epochs,   Validation loss:1.46442,   Best loss:1.46442,   Accuracy:43.33%/ 45.53%\n",
      "  14 epochs,   Validation loss:1.45935,   Best loss:1.45935,   Accuracy:44.00%/ 45.77%\n",
      "  15 epochs,   Validation loss:1.45447,   Best loss:1.45447,   Accuracy:44.67%/ 45.94%\n",
      "  16 epochs,   Validation loss:1.44984,   Best loss:1.44984,   Accuracy:44.67%/ 46.25%\n",
      "  17 epochs,   Validation loss:1.44553,   Best loss:1.44553,   Accuracy:44.67%/ 46.64%\n",
      "  18 epochs,   Validation loss:1.44153,   Best loss:1.44153,   Accuracy:44.67%/ 46.68%\n",
      "  19 epochs,   Validation loss:1.43786,   Best loss:1.43786,   Accuracy:46.00%/ 47.13%\n",
      "  20 epochs,   Validation loss:1.43447,   Best loss:1.43447,   Accuracy:46.00%/ 47.38%\n",
      "  21 epochs,   Validation loss:1.43135,   Best loss:1.43135,   Accuracy:46.00%/ 47.52%\n",
      "  22 epochs,   Validation loss:1.42848,   Best loss:1.42848,   Accuracy:46.00%/ 47.58%\n",
      "  23 epochs,   Validation loss:1.42585,   Best loss:1.42585,   Accuracy:46.67%/ 47.64%\n",
      "  24 epochs,   Validation loss:1.42345,   Best loss:1.42345,   Accuracy:47.33%/ 47.83%\n",
      "  25 epochs,   Validation loss:1.42126,   Best loss:1.42126,   Accuracy:48.00%/ 47.99%\n",
      "  26 epochs,   Validation loss:1.41924,   Best loss:1.41924,   Accuracy:48.00%/ 48.04%\n",
      "  27 epochs,   Validation loss:1.41735,   Best loss:1.41735,   Accuracy:48.00%/ 48.24%\n",
      "  28 epochs,   Validation loss:1.41553,   Best loss:1.41553,   Accuracy:48.00%/ 48.41%\n",
      "  29 epochs,   Validation loss:1.41374,   Best loss:1.41374,   Accuracy:48.00%/ 48.55%\n",
      "  30 epochs,   Validation loss:1.41190,   Best loss:1.41190,   Accuracy:48.67%/ 48.67%\n",
      "  31 epochs,   Validation loss:1.40996,   Best loss:1.40996,   Accuracy:48.67%/ 48.82%\n",
      "  32 epochs,   Validation loss:1.40782,   Best loss:1.40782,   Accuracy:48.67%/ 49.04%\n",
      "  33 epochs,   Validation loss:1.40535,   Best loss:1.40535,   Accuracy:48.67%/ 49.25%\n",
      "  34 epochs,   Validation loss:1.40240,   Best loss:1.40240,   Accuracy:48.67%/ 49.48%\n",
      "  35 epochs,   Validation loss:1.39883,   Best loss:1.39883,   Accuracy:49.33%/ 49.58%\n",
      "  36 epochs,   Validation loss:1.39451,   Best loss:1.39451,   Accuracy:49.33%/ 49.83%\n",
      "  37 epochs,   Validation loss:1.38947,   Best loss:1.38947,   Accuracy:50.00%/ 49.87%\n",
      "  38 epochs,   Validation loss:1.38390,   Best loss:1.38390,   Accuracy:50.00%/ 50.01%\n",
      "  39 epochs,   Validation loss:1.37816,   Best loss:1.37816,   Accuracy:52.67%/ 50.20%\n",
      "  40 epochs,   Validation loss:1.37257,   Best loss:1.37257,   Accuracy:53.33%/ 50.36%\n",
      "  41 epochs,   Validation loss:1.36725,   Best loss:1.36725,   Accuracy:53.33%/ 50.73%\n",
      "  42 epochs,   Validation loss:1.36213,   Best loss:1.36213,   Accuracy:53.33%/ 51.08%\n",
      "  43 epochs,   Validation loss:1.35716,   Best loss:1.35716,   Accuracy:54.00%/ 51.39%\n",
      "  44 epochs,   Validation loss:1.35249,   Best loss:1.35249,   Accuracy:54.67%/ 51.90%\n",
      "  45 epochs,   Validation loss:1.34834,   Best loss:1.34834,   Accuracy:55.33%/ 52.33%\n",
      "  46 epochs,   Validation loss:1.34467,   Best loss:1.34467,   Accuracy:55.33%/ 52.68%\n",
      "  47 epochs,   Validation loss:1.34107,   Best loss:1.34107,   Accuracy:55.33%/ 53.03%\n",
      "  48 epochs,   Validation loss:1.33703,   Best loss:1.33703,   Accuracy:55.33%/ 53.43%\n",
      "  49 epochs,   Validation loss:1.33235,   Best loss:1.33235,   Accuracy:56.67%/ 53.77%\n",
      "  50 epochs,   Validation loss:1.32725,   Best loss:1.32725,   Accuracy:57.33%/ 54.29%\n",
      "  51 epochs,   Validation loss:1.32188,   Best loss:1.32188,   Accuracy:58.00%/ 54.82%\n",
      "  52 epochs,   Validation loss:1.31664,   Best loss:1.31664,   Accuracy:59.33%/ 55.21%\n",
      "  53 epochs,   Validation loss:1.31222,   Best loss:1.31222,   Accuracy:59.33%/ 55.50%\n",
      "  54 epochs,   Validation loss:1.30847,   Best loss:1.30847,   Accuracy:58.67%/ 56.14%\n",
      "  55 epochs,   Validation loss:1.30486,   Best loss:1.30486,   Accuracy:59.33%/ 56.72%\n",
      "  56 epochs,   Validation loss:1.30109,   Best loss:1.30109,   Accuracy:59.33%/ 57.21%\n",
      "  57 epochs,   Validation loss:1.29697,   Best loss:1.29697,   Accuracy:58.67%/ 57.56%\n",
      "  58 epochs,   Validation loss:1.29231,   Best loss:1.29231,   Accuracy:59.33%/ 58.22%\n",
      "  59 epochs,   Validation loss:1.28676,   Best loss:1.28676,   Accuracy:60.00%/ 58.73%\n",
      "  60 epochs,   Validation loss:1.27995,   Best loss:1.27995,   Accuracy:61.33%/ 59.35%\n",
      "  61 epochs,   Validation loss:1.27175,   Best loss:1.27175,   Accuracy:63.33%/ 60.03%\n",
      "  62 epochs,   Validation loss:1.26271,   Best loss:1.26271,   Accuracy:64.67%/ 60.56%\n",
      "  63 epochs,   Validation loss:1.25406,   Best loss:1.25406,   Accuracy:65.33%/ 61.00%\n",
      "  64 epochs,   Validation loss:1.24684,   Best loss:1.24684,   Accuracy:66.67%/ 61.65%\n",
      "  65 epochs,   Validation loss:1.24116,   Best loss:1.24116,   Accuracy:66.67%/ 62.13%\n",
      "  66 epochs,   Validation loss:1.23651,   Best loss:1.23651,   Accuracy:66.67%/ 62.62%\n",
      "  67 epochs,   Validation loss:1.23249,   Best loss:1.23249,   Accuracy:67.33%/ 63.16%\n",
      "  68 epochs,   Validation loss:1.22895,   Best loss:1.22895,   Accuracy:68.00%/ 63.65%\n",
      "  69 epochs,   Validation loss:1.22587,   Best loss:1.22587,   Accuracy:68.67%/ 63.81%\n",
      "  70 epochs,   Validation loss:1.22322,   Best loss:1.22322,   Accuracy:68.67%/ 64.18%\n",
      "  71 epochs,   Validation loss:1.22097,   Best loss:1.22097,   Accuracy:68.67%/ 64.64%\n",
      "  72 epochs,   Validation loss:1.21904,   Best loss:1.21904,   Accuracy:68.67%/ 64.97%\n",
      "  73 epochs,   Validation loss:1.21735,   Best loss:1.21735,   Accuracy:69.33%/ 65.46%\n",
      "  74 epochs,   Validation loss:1.21584,   Best loss:1.21584,   Accuracy:70.00%/ 65.77%\n",
      "  75 epochs,   Validation loss:1.21450,   Best loss:1.21450,   Accuracy:70.00%/ 66.12%\n",
      "  76 epochs,   Validation loss:1.21343,   Best loss:1.21343,   Accuracy:71.33%/ 66.61%\n",
      "  77 epochs,   Validation loss:1.21273,   Best loss:1.21273,   Accuracy:71.33%/ 66.84%\n",
      "  78 epochs,   Validation loss:1.21239,   Best loss:1.21239,   Accuracy:70.67%/ 67.02%\n",
      "  79 epochs,   Validation loss:1.21227,   Best loss:1.21227,   Accuracy:70.67%/ 67.15%\n",
      "  80 epochs,   Validation loss:1.21224,   Best loss:1.21224,   Accuracy:70.67%/ 67.31%\n",
      "  81 epochs,   Validation loss:1.21218,   Best loss:1.21218,   Accuracy:70.67%/ 67.62%\n",
      "  82 epochs,   Validation loss:1.21200,   Best loss:1.21200,   Accuracy:70.67%/ 67.87%\n",
      "  83 epochs,   Validation loss:1.21165,   Best loss:1.21165,   Accuracy:70.67%/ 67.95%\n",
      "  84 epochs,   Validation loss:1.21112,   Best loss:1.21112,   Accuracy:70.67%/ 68.11%\n",
      "  85 epochs,   Validation loss:1.21041,   Best loss:1.21041,   Accuracy:70.67%/ 68.13%\n",
      "  86 epochs,   Validation loss:1.20953,   Best loss:1.20953,   Accuracy:70.67%/ 68.26%\n",
      "  87 epochs,   Validation loss:1.20849,   Best loss:1.20849,   Accuracy:71.33%/ 68.30%\n",
      "  88 epochs,   Validation loss:1.20733,   Best loss:1.20733,   Accuracy:71.33%/ 68.44%\n",
      "  89 epochs,   Validation loss:1.20605,   Best loss:1.20605,   Accuracy:71.33%/ 68.50%\n",
      "  90 epochs,   Validation loss:1.20469,   Best loss:1.20469,   Accuracy:71.33%/ 68.63%\n",
      "  91 epochs,   Validation loss:1.20325,   Best loss:1.20325,   Accuracy:70.67%/ 68.59%\n",
      "  92 epochs,   Validation loss:1.20175,   Best loss:1.20175,   Accuracy:70.67%/ 68.67%\n",
      "  93 epochs,   Validation loss:1.20021,   Best loss:1.20021,   Accuracy:70.67%/ 68.71%\n",
      "  94 epochs,   Validation loss:1.19866,   Best loss:1.19866,   Accuracy:70.67%/ 68.71%\n",
      "  95 epochs,   Validation loss:1.19713,   Best loss:1.19713,   Accuracy:70.67%/ 68.90%\n",
      "  96 epochs,   Validation loss:1.19564,   Best loss:1.19564,   Accuracy:70.67%/ 69.08%\n",
      "  97 epochs,   Validation loss:1.19423,   Best loss:1.19423,   Accuracy:72.00%/ 69.31%\n",
      "  98 epochs,   Validation loss:1.19291,   Best loss:1.19291,   Accuracy:72.00%/ 69.37%\n",
      "  99 epochs,   Validation loss:1.19169,   Best loss:1.19169,   Accuracy:72.00%/ 69.47%\n",
      " 100 epochs,   Validation loss:1.19056,   Best loss:1.19056,   Accuracy:73.33%/ 69.51%\n",
      " 101 epochs,   Validation loss:1.18951,   Best loss:1.18951,   Accuracy:73.33%/ 69.59%\n",
      " 102 epochs,   Validation loss:1.18849,   Best loss:1.18849,   Accuracy:74.00%/ 69.80%\n",
      " 103 epochs,   Validation loss:1.18748,   Best loss:1.18748,   Accuracy:74.00%/ 69.90%\n",
      " 104 epochs,   Validation loss:1.18643,   Best loss:1.18643,   Accuracy:74.00%/ 69.97%\n",
      " 105 epochs,   Validation loss:1.18534,   Best loss:1.18534,   Accuracy:74.00%/ 70.09%\n",
      " 106 epochs,   Validation loss:1.18418,   Best loss:1.18418,   Accuracy:74.00%/ 70.25%\n",
      " 107 epochs,   Validation loss:1.18297,   Best loss:1.18297,   Accuracy:74.00%/ 70.36%\n",
      " 108 epochs,   Validation loss:1.18172,   Best loss:1.18172,   Accuracy:74.00%/ 70.54%\n",
      " 109 epochs,   Validation loss:1.18047,   Best loss:1.18047,   Accuracy:74.00%/ 70.71%\n",
      " 110 epochs,   Validation loss:1.17924,   Best loss:1.17924,   Accuracy:74.67%/ 70.77%\n",
      " 111 epochs,   Validation loss:1.17805,   Best loss:1.17805,   Accuracy:74.67%/ 70.91%\n",
      " 112 epochs,   Validation loss:1.17693,   Best loss:1.17693,   Accuracy:74.67%/ 71.10%\n",
      " 113 epochs,   Validation loss:1.17589,   Best loss:1.17589,   Accuracy:74.67%/ 71.28%\n",
      " 114 epochs,   Validation loss:1.17494,   Best loss:1.17494,   Accuracy:74.67%/ 71.30%\n",
      " 115 epochs,   Validation loss:1.17407,   Best loss:1.17407,   Accuracy:74.67%/ 71.47%\n",
      " 116 epochs,   Validation loss:1.17328,   Best loss:1.17328,   Accuracy:74.67%/ 71.61%\n",
      " 117 epochs,   Validation loss:1.17256,   Best loss:1.17256,   Accuracy:74.67%/ 71.67%\n",
      " 118 epochs,   Validation loss:1.17189,   Best loss:1.17189,   Accuracy:74.67%/ 71.86%\n",
      " 119 epochs,   Validation loss:1.17126,   Best loss:1.17126,   Accuracy:74.67%/ 71.92%\n",
      " 120 epochs,   Validation loss:1.17064,   Best loss:1.17064,   Accuracy:74.00%/ 71.94%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 121 epochs,   Validation loss:1.17002,   Best loss:1.17002,   Accuracy:74.00%/ 72.00%\n",
      " 122 epochs,   Validation loss:1.16937,   Best loss:1.16937,   Accuracy:74.00%/ 72.17%\n",
      " 123 epochs,   Validation loss:1.16865,   Best loss:1.16865,   Accuracy:74.00%/ 72.25%\n",
      " 124 epochs,   Validation loss:1.16786,   Best loss:1.16786,   Accuracy:74.00%/ 72.37%\n",
      " 125 epochs,   Validation loss:1.16698,   Best loss:1.16698,   Accuracy:74.00%/ 72.43%\n",
      " 126 epochs,   Validation loss:1.16598,   Best loss:1.16598,   Accuracy:74.00%/ 72.58%\n",
      " 127 epochs,   Validation loss:1.16488,   Best loss:1.16488,   Accuracy:74.00%/ 72.64%\n",
      " 128 epochs,   Validation loss:1.16367,   Best loss:1.16367,   Accuracy:74.00%/ 72.78%\n",
      " 129 epochs,   Validation loss:1.16236,   Best loss:1.16236,   Accuracy:74.67%/ 72.99%\n",
      "\n",
      "Training Time:0.658595\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "no_update_tolerant = 20\n",
    "\n",
    "no_update_progress = 0\n",
    "current_best_accuracy = 0.0\n",
    "current_best_loss = 10.0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    restore_saver = tf.train.import_meta_graph(\"./Models/yen/mnist_5.ckpt.meta\") # load meta graph\n",
    "    restore_saver.restore(sess, tf.train.latest_checkpoint('./Models/yen/')) # restore weights\n",
    "#     sess.run(tf.global_variables_initializer()) # initialize the weights \n",
    "#     sess.run(tf.local_variables_initializer()) # initialize the local variables hidden in the tf.metrics.recallmethod.\n",
    "\n",
    "    \"\"\"\n",
    "    Graph Define Step\n",
    "    \n",
    "    1. Restore the variable about the component in graph which help to retrieve and assign value\n",
    "    \"\"\"\n",
    "    graph = tf.get_default_graph() # get original graph\n",
    "    X = graph.get_tensor_by_name(\"Input_X:0\") # assign placeholder variables with input X\n",
    "    y = graph.get_tensor_by_name(\"Input_Label:0\")# assign placeholder variables with input Label\n",
    "    loss = graph.get_tensor_by_name(\"Loss:0\") # assign variables about loss\n",
    "    Y_proba = graph.get_tensor_by_name(\"Softmax_Layer/Softmax:0\") # assign variables about loss\n",
    "    accuracy = graph.get_tensor_by_name(\"Accuracy:0\") # assign variables  about accuracy\n",
    "    \n",
    "    fifth_layer_out = graph.get_tensor_by_name(\"Fully_Connected_Layer_5/Elu:0\").op.outputs[0]\n",
    "#     print(fifth_layer_out)\n",
    "#     fifth_layer_out = graph.get_tensor_by_name(\"Softmax_Layer/kernel:0\").op.inputs # assign variables about loss\n",
    "#     print(fifth_layer_out)\n",
    "    \n",
    "    \"\"\"\n",
    "    2. Get the softmax layer which is what we would like to train\n",
    "    \"\"\"\n",
    "    output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Softmax_Layer\") # get softmax layer by scope name\n",
    "    \n",
    "    \"\"\"\n",
    "    3. Define the optimizer and assign to only update on softmax_layer which defined above\n",
    "    \"\"\"\n",
    "    optimizer = tf.train.AdamOptimizer(0.0017, name=\"Adam_Op\")\n",
    "    training_op = optimizer.minimize(loss, var_list=output_layer_vars) # minimize the loss and only update certern variables\n",
    "    var_list_init = tf.variables_initializer([optimizer.get_slot(var, name) for name in optimizer.get_slot_names() for var in output_layer_vars]+list(optimizer._get_beta_accumulators()))\n",
    "\n",
    "    sess.run(var_list_init)\n",
    "    \n",
    "    \"\"\"\n",
    "    Get the training output of fifth layer\n",
    "    \"\"\"\n",
    "    fifth_layer_out_cache = sess.run(fifth_layer_out, feed_dict={X:X_train2, y:y_train2})\n",
    "    print(fifth_layer_out_cache.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    Training Step\n",
    "    \"\"\"\n",
    "    begin_time = time.time()\n",
    "    for epochs in range(1000): # training on 1000 epochs\n",
    "        _, train_loss = sess.run([training_op, loss], feed_dict={fifth_layer_out:fifth_layer_out_cache, y:y_train2}) # feed with training variables\n",
    "        valid_loss, valid_accuracy = sess.run([loss, accuracy], feed_dict={X:X_valid2, y:y_valid2}) # feed with validation variables\n",
    "        test_loss ,  test_accuracy = sess.run([loss, accuracy], feed_dict={X:X_test2, y:y_test2}) # feed with testing variables\n",
    "        \n",
    "        if valid_accuracy > current_best_accuracy: # if better\n",
    "            current_best_accuracy = valid_accuracy # keep record\n",
    "            no_update_progress = 0\n",
    "        else:                                      # if not better\n",
    "            no_update_progress += 1                # record on no update\n",
    "\n",
    "        if valid_loss < current_best_loss: # if better\n",
    "            current_best_loss = valid_loss # keep record            \n",
    "            \n",
    "        if no_update_progress == no_update_tolerant: break # stop training\n",
    "            \n",
    "                \n",
    "        print(\"{:4d} epochs,   Validation loss:{:.5f},   Best loss:{:.5f},   Accuracy:{:.2f}%/ {:.2f}%\".format(\n",
    "            epochs+1,\n",
    "            valid_loss,\n",
    "            current_best_loss,                                                                                       \n",
    "            valid_accuracy*100,\n",
    "            test_accuracy*100\n",
    "        ))\n",
    "    finish_itme = time.time()\n",
    "    print(\"\\nTraining Time:{:3f}\".format(finish_itme-begin_time))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Train on 5th Layer + Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Models/yen/mnist_5.ckpt\n",
      "   1 epochs,   Validation loss:1.65729,   Best loss:1.65729,   Accuracy:20.00%/ 25.61%\n",
      "   2 epochs,   Validation loss:1.64555,   Best loss:1.64555,   Accuracy:22.67%/ 26.85%\n",
      "   3 epochs,   Validation loss:1.63323,   Best loss:1.63323,   Accuracy:23.33%/ 28.22%\n",
      "   4 epochs,   Validation loss:1.62069,   Best loss:1.62069,   Accuracy:25.33%/ 29.62%\n",
      "   5 epochs,   Validation loss:1.60814,   Best loss:1.60814,   Accuracy:25.33%/ 30.98%\n",
      "   6 epochs,   Validation loss:1.59560,   Best loss:1.59560,   Accuracy:26.00%/ 32.24%\n",
      "   7 epochs,   Validation loss:1.58288,   Best loss:1.58288,   Accuracy:26.00%/ 33.70%\n",
      "   8 epochs,   Validation loss:1.56975,   Best loss:1.56975,   Accuracy:27.33%/ 35.10%\n",
      "   9 epochs,   Validation loss:1.55610,   Best loss:1.55610,   Accuracy:28.67%/ 36.54%\n",
      "  10 epochs,   Validation loss:1.54198,   Best loss:1.54198,   Accuracy:30.67%/ 37.83%\n",
      "  11 epochs,   Validation loss:1.52762,   Best loss:1.52762,   Accuracy:35.33%/ 38.78%\n",
      "  12 epochs,   Validation loss:1.51334,   Best loss:1.51334,   Accuracy:36.67%/ 39.83%\n",
      "  13 epochs,   Validation loss:1.49932,   Best loss:1.49932,   Accuracy:38.67%/ 40.59%\n",
      "  14 epochs,   Validation loss:1.48558,   Best loss:1.48558,   Accuracy:40.00%/ 40.98%\n",
      "  15 epochs,   Validation loss:1.47196,   Best loss:1.47196,   Accuracy:40.00%/ 41.66%\n",
      "  16 epochs,   Validation loss:1.45832,   Best loss:1.45832,   Accuracy:41.33%/ 42.30%\n",
      "  17 epochs,   Validation loss:1.44452,   Best loss:1.44452,   Accuracy:44.67%/ 42.93%\n",
      "  18 epochs,   Validation loss:1.43053,   Best loss:1.43053,   Accuracy:45.33%/ 43.88%\n",
      "  19 epochs,   Validation loss:1.41636,   Best loss:1.41636,   Accuracy:46.00%/ 44.95%\n",
      "  20 epochs,   Validation loss:1.40210,   Best loss:1.40210,   Accuracy:48.00%/ 46.00%\n",
      "  21 epochs,   Validation loss:1.38789,   Best loss:1.38789,   Accuracy:50.00%/ 47.50%\n",
      "  22 epochs,   Validation loss:1.37391,   Best loss:1.37391,   Accuracy:53.33%/ 48.63%\n",
      "  23 epochs,   Validation loss:1.36037,   Best loss:1.36037,   Accuracy:56.00%/ 50.24%\n",
      "  24 epochs,   Validation loss:1.34749,   Best loss:1.34749,   Accuracy:57.33%/ 51.88%\n",
      "  25 epochs,   Validation loss:1.33543,   Best loss:1.33543,   Accuracy:60.00%/ 53.28%\n",
      "  26 epochs,   Validation loss:1.32427,   Best loss:1.32427,   Accuracy:61.33%/ 54.97%\n",
      "  27 epochs,   Validation loss:1.31402,   Best loss:1.31402,   Accuracy:60.67%/ 56.28%\n",
      "  28 epochs,   Validation loss:1.30460,   Best loss:1.30460,   Accuracy:61.33%/ 57.21%\n",
      "  29 epochs,   Validation loss:1.29585,   Best loss:1.29585,   Accuracy:62.67%/ 58.36%\n",
      "  30 epochs,   Validation loss:1.28760,   Best loss:1.28760,   Accuracy:63.33%/ 59.12%\n",
      "  31 epochs,   Validation loss:1.27965,   Best loss:1.27965,   Accuracy:63.33%/ 59.84%\n",
      "  32 epochs,   Validation loss:1.27186,   Best loss:1.27186,   Accuracy:64.67%/ 60.48%\n",
      "  33 epochs,   Validation loss:1.26411,   Best loss:1.26411,   Accuracy:66.00%/ 61.30%\n",
      "  34 epochs,   Validation loss:1.25631,   Best loss:1.25631,   Accuracy:68.00%/ 61.96%\n",
      "  35 epochs,   Validation loss:1.24843,   Best loss:1.24843,   Accuracy:68.00%/ 62.50%\n",
      "  36 epochs,   Validation loss:1.24049,   Best loss:1.24049,   Accuracy:68.00%/ 63.30%\n",
      "  37 epochs,   Validation loss:1.23256,   Best loss:1.23256,   Accuracy:69.33%/ 64.02%\n",
      "  38 epochs,   Validation loss:1.22478,   Best loss:1.22478,   Accuracy:70.00%/ 64.64%\n",
      "  39 epochs,   Validation loss:1.21727,   Best loss:1.21727,   Accuracy:70.67%/ 65.46%\n",
      "  40 epochs,   Validation loss:1.21012,   Best loss:1.21012,   Accuracy:72.67%/ 66.16%\n",
      "  41 epochs,   Validation loss:1.20341,   Best loss:1.20341,   Accuracy:72.67%/ 66.94%\n",
      "  42 epochs,   Validation loss:1.19717,   Best loss:1.19717,   Accuracy:72.67%/ 67.72%\n",
      "  43 epochs,   Validation loss:1.19142,   Best loss:1.19142,   Accuracy:72.67%/ 68.05%\n",
      "  44 epochs,   Validation loss:1.18618,   Best loss:1.18618,   Accuracy:74.67%/ 68.69%\n",
      "  45 epochs,   Validation loss:1.18143,   Best loss:1.18143,   Accuracy:74.00%/ 69.35%\n",
      "  46 epochs,   Validation loss:1.17715,   Best loss:1.17715,   Accuracy:74.00%/ 69.80%\n",
      "  47 epochs,   Validation loss:1.17329,   Best loss:1.17329,   Accuracy:74.00%/ 70.13%\n",
      "  48 epochs,   Validation loss:1.16981,   Best loss:1.16981,   Accuracy:74.67%/ 70.34%\n",
      "  49 epochs,   Validation loss:1.16667,   Best loss:1.16667,   Accuracy:74.67%/ 70.73%\n",
      "  50 epochs,   Validation loss:1.16382,   Best loss:1.16382,   Accuracy:75.33%/ 71.20%\n",
      "  51 epochs,   Validation loss:1.16124,   Best loss:1.16124,   Accuracy:76.00%/ 71.57%\n",
      "  52 epochs,   Validation loss:1.15890,   Best loss:1.15890,   Accuracy:76.67%/ 71.80%\n",
      "  53 epochs,   Validation loss:1.15678,   Best loss:1.15678,   Accuracy:76.67%/ 71.94%\n",
      "  54 epochs,   Validation loss:1.15486,   Best loss:1.15486,   Accuracy:76.67%/ 72.21%\n",
      "  55 epochs,   Validation loss:1.15313,   Best loss:1.15313,   Accuracy:76.67%/ 72.31%\n",
      "  56 epochs,   Validation loss:1.15157,   Best loss:1.15157,   Accuracy:76.67%/ 72.52%\n",
      "  57 epochs,   Validation loss:1.15017,   Best loss:1.15017,   Accuracy:77.33%/ 72.70%\n",
      "  58 epochs,   Validation loss:1.14891,   Best loss:1.14891,   Accuracy:78.00%/ 72.74%\n",
      "  59 epochs,   Validation loss:1.14777,   Best loss:1.14777,   Accuracy:78.00%/ 72.87%\n",
      "  60 epochs,   Validation loss:1.14673,   Best loss:1.14673,   Accuracy:78.00%/ 72.89%\n",
      "  61 epochs,   Validation loss:1.14580,   Best loss:1.14580,   Accuracy:78.00%/ 72.87%\n",
      "  62 epochs,   Validation loss:1.14494,   Best loss:1.14494,   Accuracy:77.33%/ 72.99%\n",
      "  63 epochs,   Validation loss:1.14415,   Best loss:1.14415,   Accuracy:77.33%/ 73.11%\n",
      "  64 epochs,   Validation loss:1.14343,   Best loss:1.14343,   Accuracy:77.33%/ 73.32%\n",
      "  65 epochs,   Validation loss:1.14275,   Best loss:1.14275,   Accuracy:77.33%/ 73.42%\n",
      "  66 epochs,   Validation loss:1.14211,   Best loss:1.14211,   Accuracy:77.33%/ 73.34%\n",
      "  67 epochs,   Validation loss:1.14151,   Best loss:1.14151,   Accuracy:77.33%/ 73.46%\n",
      "  68 epochs,   Validation loss:1.14093,   Best loss:1.14093,   Accuracy:77.33%/ 73.52%\n",
      "  69 epochs,   Validation loss:1.14038,   Best loss:1.14038,   Accuracy:77.33%/ 73.67%\n",
      "  70 epochs,   Validation loss:1.13985,   Best loss:1.13985,   Accuracy:77.33%/ 73.71%\n",
      "  71 epochs,   Validation loss:1.13933,   Best loss:1.13933,   Accuracy:77.33%/ 73.79%\n",
      "  72 epochs,   Validation loss:1.13883,   Best loss:1.13883,   Accuracy:77.33%/ 73.94%\n",
      "  73 epochs,   Validation loss:1.13834,   Best loss:1.13834,   Accuracy:77.33%/ 74.02%\n",
      "  74 epochs,   Validation loss:1.13786,   Best loss:1.13786,   Accuracy:78.00%/ 74.02%\n",
      "  75 epochs,   Validation loss:1.13739,   Best loss:1.13739,   Accuracy:78.00%/ 74.10%\n",
      "  76 epochs,   Validation loss:1.13692,   Best loss:1.13692,   Accuracy:78.00%/ 74.10%\n",
      "  77 epochs,   Validation loss:1.13646,   Best loss:1.13646,   Accuracy:78.00%/ 74.20%\n",
      "\n",
      "Training Time:0.391235\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "no_update_tolerant = 20\n",
    "\n",
    "no_update_progress = 0\n",
    "current_best_accuracy = 0.0\n",
    "current_best_loss = 10.0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    restore_saver = tf.train.import_meta_graph(\"./Models/yen/mnist_5.ckpt.meta\") # load meta graph\n",
    "    restore_saver.restore(sess, tf.train.latest_checkpoint('./Models/yen/')) # restore weights\n",
    "#     sess.run(tf.global_variables_initializer()) # initialize the weights \n",
    "#     sess.run(tf.local_variables_initializer()) # initialize the local variables hidden in the tf.metrics.recallmethod.\n",
    "\n",
    "    \"\"\"\n",
    "    Graph Define Step\n",
    "    \n",
    "    1. Restore the variable about the component in graph which help to retrieve and assign value\n",
    "    \"\"\"\n",
    "    graph = tf.get_default_graph() # get original graph\n",
    "    X = graph.get_tensor_by_name(\"Input_X:0\") # assign placeholder variables with input X\n",
    "    y = graph.get_tensor_by_name(\"Input_Label:0\")# assign placeholder variables with input Label\n",
    "    \n",
    "    fourth_layer_out = graph.get_tensor_by_name(\"Fully_Connected_Layer_4/Elu:0\").op.outputs[0]\n",
    "#     print(fourth_layer_out)\n",
    "    \n",
    "    \"\"\"\n",
    "    2. Get the training output of fourth layer\n",
    "    \"\"\"\n",
    "    fourth_layer_out_cache = sess.run(fourth_layer_out, feed_dict={X:X_train2, y:y_train2}) # get the output of fourth layer\n",
    "#     print(fourth_layer_out_cache.shape)\n",
    "    temp = set(tf.global_variables())\n",
    "    \"\"\"\n",
    "    3.Create new softmax layer and connect old fourth layer, connect to softmax layer\n",
    "    \"\"\"\n",
    "    y_ = tf.layers.dense(inputs=fourth_layer_out, \n",
    "                             units=5, \n",
    "                             activation=tf.nn.softmax, #softmax\n",
    "                             kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                             name = \"New_Softmax_Layer\"\n",
    "                            )  # Layer using dense\n",
    "\n",
    "    \"\"\"\n",
    "    4. Get the softmax layer which is what we would like to train\n",
    "    \"\"\"\n",
    "    output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"New_Softmax_Layer\") # get softmax layer by scope name\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    5. Define loss, optimizer and assign to only update on softmax_layer which defined above\n",
    "    \"\"\"   \n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_, name=\"New_Cross_Entropy\"), name=\"New_Loss\") # Refine Loss Function\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.0017, name=\"Adam_Op\")\n",
    "    training_op = optimizer.minimize(loss, var_list=output_layer_vars) # minimize the loss and only update certern variables\n",
    "    \n",
    "    \"\"\"\n",
    "    6. Define new evaluation metrics\n",
    "    \"\"\"\n",
    "    # Define Accuracy\n",
    "    predicted_class = tf.argmax(y_, 1, output_type=tf.int32)\n",
    "    correct_predict = tf.equal(y, predicted_class) # [True, False ..., True]\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32)) # [True, False ..., True] --> [1,0,...,1]\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    7. Initialize all the variables\n",
    "    \"\"\"\n",
    "    var_list_init = tf.variables_initializer(set(tf.global_variables())-temp)\n",
    "    sess.run(var_list_init) # init the variable about fourth + softmax layer + adam optimizer\n",
    "   \n",
    "    \n",
    "    \"\"\"\n",
    "    Training Step\n",
    "    \"\"\"\n",
    "    begin_time = time.time()\n",
    "    for epochs in range(1000): # training on 1000 epochs\n",
    "        _, train_loss = sess.run([training_op, loss], feed_dict={fourth_layer_out:fourth_layer_out_cache, y:y_train2}) # feed with training variables\n",
    "        valid_loss, valid_accuracy = sess.run([loss, accuracy], feed_dict={X:X_valid2, y:y_valid2}) # feed with validation variables\n",
    "        test_loss ,  test_accuracy = sess.run([loss, accuracy], feed_dict={X:X_test2, y:y_test2}) # feed with testing variables\n",
    "        \n",
    "        if valid_accuracy > current_best_accuracy: # if better\n",
    "            current_best_accuracy = valid_accuracy # keep record\n",
    "            no_update_progress = 0\n",
    "        else:                                      # if not better\n",
    "            no_update_progress += 1                # record on no update\n",
    "\n",
    "        if valid_loss < current_best_loss: # if better\n",
    "            current_best_loss = valid_loss # keep record            \n",
    "            \n",
    "        if no_update_progress == no_update_tolerant: break # stop training\n",
    "            \n",
    "                \n",
    "        print(\"{:4d} epochs,   Validation loss:{:.5f},   Best loss:{:.5f},   Accuracy:{:.2f}%/ {:.2f}%\".format(\n",
    "            epochs+1,\n",
    "            valid_loss,\n",
    "            current_best_loss,                                                                                       \n",
    "            valid_accuracy*100,\n",
    "            test_accuracy*100\n",
    "        ))\n",
    "    finish_itme = time.time()\n",
    "    print(\"\\nTraining Time:{:3f}\".format(finish_itme-begin_time))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'New_Softmax_Layer/kernel:0' shape=(128, 5) dtype=float32_ref>,\n",
       " <tf.Variable 'New_Softmax_Layer/bias:0' shape=(5,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
